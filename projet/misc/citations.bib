@article{martin-maroto_algebraic_2018,
	title = {Algebraic {Machine} {Learning}},
	url = {http://arxiv.org/abs/1803.05252},
	abstract = {Machine learning algorithms use error function minimization to fit a large set of parameters in a preexisting model. However, error minimization eventually leads to a memorization of the training dataset, losing the ability to generalize to other datasets. To achieve generalization something else is needed, for example a regularization method or stopping the training when error in a validation dataset is minimal. Here we propose a different approach to learning and generalization that is parameter-free, fully discrete and that does not use function minimization. We use the training data to find an algebraic representation with minimal size and maximal freedom, explicitly expressed as a product of irreducible components. This algebraic representation is shown to directly generalize, giving high accuracy in test data, more so the smaller the representation. We prove that the number of generalizing representations can be very large and the algebra only needs to find one. We also derive and test a relationship between compression and error rate. We give results for a simple problem solved step by step, hand-written character recognition, and the Queens Completion problem as an example of unsupervised learning. As an alternative to statistical learning, algebraic learning may offer advantages in combining bottom-up and top-down information, formal concept derivation from data and large-scale parallelization.},
	urldate = {2021-10-01},
	journal = {arXiv:1803.05252 [cs, math]},
	author = {Martin-Maroto, Fernando and de Polavieja, Gonzalo G.},
	month = mar,
	year = 2018,
	note = {arXiv: 1803.05252},
	keywords = {Computer Science - Machine Learning, Computer Science - Discrete Mathematics, Mathematics - Commutative Algebra, Mathematics - Rings and Algebras},
}

@article{martin-maroto_finite_2021,
	title = {Finite {Atomized} {Semilattices}},
	url = {http://arxiv.org/abs/2102.08050},
	abstract = {We show that every finite semilattice can be represented as an atomized semilattice, an algebraic structure with additional elements (atoms) that extend the semilattice's partial order. Each atom maps to one subdirectly irreducible component, and the set of atoms forms a hypergraph that fully defines the semilattice. An atomization always exists and is unique up to "redundant atoms". Atomized semilattices are representations that can be used as computational tools for building semilattice models from sentences, as well as building its subalgebras and products. Atomized semilattices can be applied to machine learning and to the study of semantic embeddings into algebras with idempotent operators.},
	urldate = {2021-10-01},
	journal = {arXiv:2102.08050 [cs, math]},
	author = {Martin-Maroto, Fernando and de Polavieja, Gonzalo G.},
	month = feb,
	year = {2021},
	note = {arXiv: 2102.08050},
	keywords = {Mathematics - Rings and Algebras, Computer Science - Discrete Mathematics, Mathematics - Combinatorics, 06-XX},
}

@article{oles_application_2000,
	title = {An application of lattice theory to knowledge representation},
	volume = {249},
	issn = {03043975},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S030439750000058X},
	doi = {10.1016/S0304-3975(00)00058-X},
	language = {en},
	number = {1},
	urldate = {2021-10-01},
	journal = {Theoretical Computer Science},
	author = {Oles, Frank J.},
	month = oct,
	year = {2000},
	pages = {163--196},
}

@article{stone_theory_1936,
	title = {The {Theory} of {Representation} for {Boolean} {Algebras}},
	volume = {40},
	issn = {00029947},
	url = {https://www.jstor.org/stable/1989664?origin=crossref},
	doi = {10.2307/1989664},
	number = {1},
	urldate = {2021-10-01},
	journal = {Transactions of the American Mathematical Society},
	author = {Stone, M. H.},
	month = jul,
	year = {1936},
	pages = {37},
}

@book{burris_course_1981,
	address = {New York, NY},
	series = {Graduate {Texts} in {Mathematics}},
	title = {A {Course} in {Universal} {Algebra}},
	volume = {78},
	isbn = {9781461381327 9781461381303},
	url = {http://link.springer.com/10.1007/978-1-4613-8130-3},
	urldate = {2021-10-01},
	publisher = {Springer New York},
	author = {Burris, Stanley and Sankappanavar, H. P.},
	year = {1981},
	doi = {10.1007/978-1-4613-8130-3},
}

@book{davey_introduction_2002,
	address = {Cambridge, UK ; New York, NY},
	edition = {2nd ed},
	title = {Introduction to lattices and order},
	isbn = {9780521784511},
	publisher = {Cambridge University Press},
	author = {Davey, B. A. and Priestley, H. A.},
	year = {2002},
	keywords = {Lattice theory},
}

@article{papert_congruence_1964,
	title = {Congruence {Relations} in {Semi}-{Lattices}},
	volume = {s1-39},
	issn = {00246107},
	url = {http://doi.wiley.com/10.1112/jlms/s1-39.1.723},
	doi = {10.1112/jlms/s1-39.1.723},
	language = {en},
	number = {1},
	urldate = {2021-10-01},
	journal = {Journal of the London Mathematical Society},
	author = {Papert, Dona},
	year = {1964},
	pages = {723--729},
}

@article{you_deep_2017,
	title = {Deep {Lattice} {Networks} and {Partial} {Monotonic} {Functions}},
	url = {http://arxiv.org/abs/1709.06680},
	abstract = {We propose learning deep models that are monotonic with respect to a user-specified set of inputs by alternating layers of linear embeddings, ensembles of lattices, and calibrators (piecewise linear functions), with appropriate constraints for monotonicity, and jointly training the resulting network. We implement the layers and projections with new computational graph nodes in TensorFlow and use the ADAM optimizer and batched stochastic gradients. Experiments on benchmark and real-world datasets show that six-layer monotonic deep lattice networks achieve state-of-the art performance for classification and regression with monotonicity guarantees.},
	urldate = {2021-10-01},
	journal = {arXiv:1709.06680 [cs, stat]},
	author = {You, Seungil and Ding, David and Canini, Kevin and Pfeifer, Jan and Gupta, Maya},
	month = sep,
	year = {2017},
	note = {arXiv: 1709.06680},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
}

@article{gupta_monotonic_2016,
	title = {Monotonic {Calibrated} {Interpolated} {Look}-{Up} {Tables}},
	url = {http://arxiv.org/abs/1505.06378},
	abstract = {Real-world machine learning applications may require functions that are fast-to-evaluate and interpretable. In particular, guaranteed monotonicity of the learned function can be critical to user trust. We propose meeting these goals for low-dimensional machine learning problems by learning flexible, monotonic functions using calibrated interpolated look-up tables. We extend the structural risk minimization framework of lattice regression to train monotonic look-up tables by solving a convex problem with appropriate linear inequality constraints. In addition, we propose jointly learning interpretable calibrations of each feature to normalize continuous features and handle categorical or missing data, at the cost of making the objective non-convex. We address large-scale learning through parallelization, mini-batching, and propose random sampling of additive regularizer terms. Case studies with real-world problems with five to sixteen features and thousands to millions of training samples demonstrate the proposed monotonic functions can achieve state-of-the-art accuracy on practical problems while providing greater transparency to users.},
	urldate = {2021-10-01},
	journal = {arXiv:1505.06378 [cs]},
	author = {Gupta, Maya and Cotter, Andrew and Pfeifer, Jan and Voevodski, Konstantin and Canini, Kevin and Mangylov, Alexander and Moczydlowski, Wojtek and van Esbroeck, Alex},
	month = jan,
	year = {2016},
	note = {arXiv: 1505.06378},
	keywords = {Computer Science - Machine Learning},
}

@article{nilsson_logic_1991,
	title = {Logic and artificial intelligence},
	volume = {47},
	issn = {00043702},
	url = {https://linkinghub.elsevier.com/retrieve/pii/000437029190049P},
	doi = {10.1016/0004-3702(91)90049-P},
	language = {en},
	number = {1-3},
	urldate = {2021-10-02},
	journal = {Artificial Intelligence},
	author = {Nilsson, Nils J.},
	month = jan,
	year = {1991},
	pages = {31--56},
}

@article{shin_single_2001,
	title = {A single trial analysis of hippocampal theta frequency during nonsteady wheel running in rats [{Brain} {Research} 897 (2001) 217-221]},
	volume = {908},
	issn = {00068993},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0006899301026440},
	doi = {10.1016/S0006-8993(01)02644-0},
	language = {en},
	number = {1},
	urldate = {2021-10-02},
	journal = {Brain Research},
	author = {Shin, Jonghan and Talnov, Arkadi},
	month = jul,
	year = {2001},
	pages = {104--105},
}

@article{thorpe_speed_1996,
	title = {Speed of processing in the human visual system},
	volume = {381},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/articles/381520a0},
	doi = {10.1038/381520a0},
	language = {en},
	number = {6582},
	urldate = {2021-10-02},
	journal = {Nature},
	author = {Thorpe, Simon and Fize, Denis and Marlot, Catherine},
	month = jun,
	year = {1996},
	pages = {520--522},
}

@book{singh_proceedings_2017,
	address = {Palo Alto, California},
	title = {Proceedings of the {Thirty}-{First} {AAAI} {Conference} on {Artificial} {Intelligence}, {Twenty}-{Ninth} {Innovative} {Applications} of {Artificial} {Intelligence} {Conference}, {Seventh} {Symposium} on {Educational} {Advances} in {Artificial} {Intelligence}: 4-9 {February} 2017, {San} {Francisco}, {California}, {USA}. {Volume} 1},
	isbn = {9781577357803},
	shorttitle = {Proceedings of the {Thirty}-{First} {AAAI} {Conference} on {Artificial} {Intelligence}, {Twenty}-{Ninth} {Innovative} {Applications} of {Artificial} {Intelligence} {Conference}, {Seventh} {Symposium} on {Educational} {Advances} in {Artificial} {Intelligence}},
	language = {eng},
	publisher = {AAAI Press},
	editor = {Singh, Satinder and Crawford, James and Eaton, Eric and {Association for the Advancement of Artificial Intelligence}},
	year = {2017},
}

@book{xu_lattice-valued_2011,
	address = {Berlin; London},
	title = {Lattice-valued logic: an alternative approach to treat fuzziness and incomparability},
	isbn = {9783642072796},
	shorttitle = {Lattice-valued logic},
	language = {English},
	publisher = {Springer},
	author = {Xu, Yang},
	year = {2011},
	note = {OCLC: 751527955},
}
